# configurations for training

batch_size: 3  # batch size on single device
n_samples: 1
epochs: 200
self_conditioning: False
predict_diff: False
checkpoint_interval: 10  # save checkpoint every n epochs
seed: 37
deterministic: False
logging_dir: "/root/autodl-tmp/Fold-Embedder/logs"  # better absolute path, to revise

resume:
  ckpt_dir: null
  load_model_only: False

data:
  path_to_dataset: "/root/autodl-tmp/pretrain_dataset/metadata.copy.csv"  # better absolute path, to revise
  path_to_ccd_info: "/root/autodl-tmp/Fold-Embedder/src/data/ccd_atom37.pkl"  # better absolute path, to revise
  truncate_size: 384
  recenter_atoms: True
  eps: 1e-8
  prop_train: 0.95
  shuffle: False
  num_workers: 16
  pin_memory: False

model:
  c_atom: 128
  c_atompair: 16
  c_token: 384
  c_s: 384
  c_z: 128
  n_atom_layers: 4
  n_token_layers: 4
  n_atom_attn_heads: 4
  n_token_attn_heads: 8
  initialization:
    zero_init_condition_transition: False
    zero_init_atom_encoder_residual_linear: False
    he_normal_init_atom_encoder_small_mlp: False
    he_normal_init_atom_encoder_output: False
    glorot_init_self_attention: False
    zero_init_adaln: True
    zero_init_residual_condition_transition: False
    zero_init_dit_output: True
    zero_init_atom_decoder_linear: False
  position_scaling: 16.0

optimizer:
  lr: 0.0005
  weight_decay: 0.
  beta1: 0.9
  beta2: 0.999
  use_adamw: False
  lr_scheduler: "af3"
  warmup_steps: 2000
  decay_every_n_steps: 40000
  decay_factor: 0.95

loss:
  weight_mse: 0.33
  eps: 1e-8
  reduction: "mean"
  lddt_enabled: True
  bond_enabled: True
  clip_grad_value: 0

hydra:
  run:
    dir: .
  output_subdir: null
